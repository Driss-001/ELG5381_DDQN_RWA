{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optical RL-Gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os,pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "\n",
    "path ='C:/Users/D-GAMING-MACHINE/Documents/ELG5381/BranchingDQN/'\n",
    "sys.path.append(path)\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.15.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# silencing tensorflow warnings\n",
    "import tensorflow as tf\n",
    "import gymnasium as gym\n",
    "import logging\n",
    "logging.getLogger('tensorflow').setLevel(logging.FATAL)\n",
    "tf.ConfigProto(device_count = {'GPU': 1})\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import stable_baselines3\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.ppo.policies import MlpPolicy\n",
    "from stable_baselines3.common import results_plotter\n",
    "stable_baselines3.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import tensorflow as  tf\n",
    "import torch  as th \n",
    "print(th.cuda.is_available())\n",
    "#print(stable_baselines3.common.utils.get_device())\n",
    "from optical_rl_gym.utils import Path, evaluate_heuristic, random_policy\n",
    "from optical_rl_gym.envs.qos_constrained_ra import MatrixObservationWithPaths\n",
    "from optical_rl_gym.envs.rwa_env import PathOnlyFirstFitAction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveOnBestTrainingRewardCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback for saving a model (the check is done every ``check_freq`` steps)\n",
    "    based on the training reward (in practice, we recommend using ``EvalCallback``).\n",
    "\n",
    "    :param check_freq: (int)\n",
    "    :param log_dir: (str) Path to the folder where the model will be saved.\n",
    "      It must contains the file created by the ``Monitor`` wrapper.\n",
    "    :param verbose: (int)\n",
    "    \"\"\"\n",
    "    def __init__(self, check_freq: int, log_dir: str, verbose=1):\n",
    "        super(SaveOnBestTrainingRewardCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.log_dir = log_dir\n",
    "        self.save_path = os.path.join(log_dir, 'best_model')\n",
    "        self.best_mean_reward = -np.inf\n",
    "\n",
    "    def _init_callback(self) -> None:\n",
    "        # Create folder if needed\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            # Retrieve training reward\n",
    "            x, y = ts2xy(load_results(self.log_dir), 'timesteps')\n",
    "            if len(x) > 0:\n",
    "                 # Mean training reward over the last 100 episodes\n",
    "                mean_reward = np.mean(y[-200:])\n",
    "                if self.verbose > 0:\n",
    "                    print(\"Num timesteps: {} - \".format(self.num_timesteps), end=\"\")\n",
    "                    print(\"Best mean reward: {:.2f} - Last mean reward per episode: {:.2f}\".format(self.best_mean_reward, mean_reward))\n",
    "                  # New best model, you could save the agent here\n",
    "                if mean_reward > self.best_mean_reward:\n",
    "                    self.best_mean_reward = mean_reward\n",
    "                    # Example for saving best model\n",
    "                    if self.verbose > 0:\n",
    "                        print(\"Saving new best model to {}\".format(self.save_path))\n",
    "                        self.model.save(self.save_path)\n",
    "                if self.verbose > 0:\n",
    "                    clear_output(wait=True)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EdgeView([('1', '2'), ('1', '3'), ('1', '8'), ('2', '3'), ('2', '4'), ('3', '6'), ('4', '5'), ('4', '11'), ('5', '6'), ('5', '7'), ('6', '10'), ('6', '14'), ('7', '8'), ('7', '10'), ('8', '9'), ('9', '10'), ('9', '12'), ('9', '13'), ('11', '12'), ('11', '13'), ('12', '14'), ('13', '14')])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topology_name = 'nsfnet_chen_eon'\n",
    "k_paths = 5\n",
    "episode_num = 100\n",
    "episode_length = 1000\n",
    "with open(f'topologies/nsfnet_chen_5-paths_6-modulations.h5', 'rb') as f:\n",
    "    topology = pickle.load(f)\n",
    "    \n",
    "\n",
    "node_num = topology.number_of_nodes()\n",
    "req_tens = 1/node_num/(node_num-1)*np.ones((node_num,node_num)) #uniform node request probability tensor\n",
    "node_request_probabilities = req_tens.dot(np.ones(node_num))\n",
    "env_args = dict(seed=10, allow_rejection=True,\n",
    "                load=450, episode_length=episode_length, num_spectrum_resources=16,\n",
    "                topology = topology,\n",
    "                node_request_probabilities = node_request_probabilities,\n",
    "                    )\n",
    "topology.edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create log dir\n",
    "log_dir = \"./tmp/rwa-ppo/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "callback = SaveOnBestTrainingRewardCallback(check_freq=100, log_dir=log_dir)\n",
    "\n",
    "env = gym.make('RWA-v0', **env_args)\n",
    "\n",
    "# uses a matrix observation with paths\n",
    "env = MatrixObservationWithPaths(env)\n",
    "\n",
    "# logs will be saved in log_dir/monitor.csv\n",
    "# in our case, on top of the usual monitored metrics, we also monitor service blocking rate\n",
    "env = Monitor(env, log_dir + 'training', info_keywords=('episode_service_blocking_rate','episode_service_accepted'))\n",
    "\n",
    "policy_args = dict(net_arch=2*[128]) # the neural network has 2 layers with 128 neurons each\n",
    "\n",
    "agent = PPO(MlpPolicy, env, verbose=0, tensorboard_log=\"./tb/PPO-RWA-v0/\", policy_kwargs=policy_args, gamma=.95, learning_rate=1.57e-5,batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 94700 - Best mean reward: 95.52 - Last mean reward per episode: 95.52\n"
     ]
    }
   ],
   "source": [
    "agent.learn(total_timesteps=episode_length*episode_num, callback=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_plotter.plot_results([log_dir], episode_num, results_plotter.X_EPISODES, \"Routing and Wavelength Assignment PPO\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python37",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
